{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2 as cv\n",
    "from keras.models import *\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D\n",
    "from keras.layers import merge \n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myUnet:\n",
    "    def __init__(self, img_rows = 200, img_cols = 200):\n",
    "        \n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "   \n",
    "    def load_data(self):\n",
    "        \n",
    "        #image_dir = 'real_images_ER'\n",
    "        image_dir = 'images_ER'\n",
    "        mask_dir = 'masks_ER'\n",
    "        list_all_img_dir = os.listdir(image_dir)\n",
    "        img_list = [i for i in list_all_img_dir if i.endswith('.tif')]\n",
    "        \n",
    "        ###\n",
    "        img_numb = len(img_list)\n",
    "        train_numb = 240\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        imgs_all = np.empty([img_numb,self.img_rows, self.img_cols,1])\n",
    "        imgs_mask_all = np.empty([img_numb,self.img_rows, self.img_cols,1])\n",
    "        \n",
    "        \n",
    "        for idx in range(img_numb):\n",
    "            filename = img_list[idx]\n",
    "            #print(idx)\n",
    "            img_path = image_dir+'/'+filename\n",
    "            img = cv.imread(img_path,0)\n",
    "            img = np.expand_dims(img,axis=2)\n",
    "            img = np.expand_dims(img,axis=0)\n",
    "            imgs_all[idx,:,:,:] = img\n",
    "            \n",
    "            mask_path = mask_dir+'/'+filename\n",
    "            img_mask = cv.imread(mask_path,0)\n",
    "            img_mask = np.expand_dims(img_mask,axis=2)\n",
    "            img_mask = np.expand_dims(img_mask,axis=0)\n",
    "            imgs_mask_all[idx,:,:,:] = img_mask\n",
    "        print('data dimension:', imgs_all.shape)\n",
    "        \n",
    "        img_max_thr = imgs_all.max()\n",
    "        imgs_all = imgs_all/img_max_thr\n",
    "        \n",
    "        imgs_mask_all[imgs_mask_all>0]=1\n",
    "        imgs_mask_all[imgs_mask_all<=0]=0\n",
    "        \n",
    "        imgs_train = imgs_all[:train_numb,:,:,:]\n",
    "        imgs_mask_train = imgs_mask_all[:train_numb,:,:,:]\n",
    "        imgs_test = imgs_all[train_numb:,:,:,:]\n",
    "        imgs_mask_test = imgs_mask_all[train_numb:,:,:,:]\n",
    "             \n",
    "        return imgs_train, imgs_mask_train, imgs_test, imgs_mask_test \n",
    "    \n",
    "    def get_unet(self):\n",
    "        inputs = Input((self.img_rows, self.img_cols,1))\n",
    "        conv1 = Conv2D(64, kernel_size=(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "            \n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "            \n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        drop3 = Dropout(0.2)(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "            \n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        drop4 = Dropout(0.2)(conv4)\n",
    "\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n",
    "        merge7 = concatenate([conv3,up7], axis = 3) \n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        merge8 = concatenate([conv2,up8], axis = 3) \n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8)) \n",
    "        merge9 = concatenate([conv1,up9], axis = 3) \n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "            \n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9) \n",
    "        model = Model(inputs = inputs, outputs = conv10) \n",
    "        model.compile(optimizer = SGD(lr = 0.3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "        #model.compile(optimizer = Adam(lr=0.0001,beta_1=0.95, beta_2=0.999), loss = 'binary_crossentropy', metrics = ['accuracy'])    \n",
    "        return model      \n",
    "        \n",
    "    def run_unet(self):\n",
    "        \n",
    "        print('Loading data')\n",
    "        imgs_train, imgs_mask_train, imgs_test, imgs_mask_test = self.load_data()\n",
    "        print('Loading data done')\n",
    "        model = self.get_unet()\n",
    "        print('Load unet model done')\n",
    "        \n",
    "        result_folder = 'result_v9'\n",
    "        \n",
    "        # Training\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "      \n",
    "        model_checkpoint = ModelCheckpoint('./'+result_folder+'/unet.hdf5',monitor='loss',verbose=1,save_best_only=True)\n",
    "        print('Fitting model...')\n",
    "        model.fit(imgs_train, imgs_mask_train,batch_size=1,epochs=15,verbose=1,shuffle=True,validation_split=0.1, callbacks=[model_checkpoint])\n",
    "  \n",
    "        # Test \n",
    "        #print('loading weights')\n",
    "        #model.load_weights('./trial1_result/unet.hdf5')\n",
    "        #print('load weights done')\n",
    "        \n",
    "        test_evaluation = model.evaluate(imgs_test,imgs_mask_test,batch_size=1,verbose=1)\n",
    "        print test_evaluation\n",
    "        test_loss,test_acc = test_evaluation\n",
    "        print('test_loss:',test_loss)\n",
    "        print('test_acc:',test_acc)\n",
    "        \n",
    "        print('Predict test data')\n",
    "        test_predict = model.predict(imgs_test,batch_size=1,verbose=1)\n",
    "        \n",
    "        np.save('./'+result_folder+'/unet_predict.npy',test_predict)\n",
    "        np.save('./'+result_folder+'/unet_imgs_test.npy',imgs_test)\n",
    "        np.save('./'+result_folder+'/unet_imgs_mask_test.npy',imgs_mask_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "('data dimension:', (258, 200, 200, 1))\n",
      "Loading data done\n",
      "Load unet model done\n",
      "Fitting model...\n",
      "Train on 216 samples, validate on 24 samples\n",
      "Epoch 1/15\n",
      "216/216 [==============================] - 12s 54ms/step - loss: 0.3814 - acc: 0.8449 - val_loss: 0.2788 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.38144, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 2/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.2355 - acc: 0.9105 - val_loss: 0.2403 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00002: loss improved from 0.38144 to 0.23550, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 3/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.2140 - acc: 0.9167 - val_loss: 0.2234 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00003: loss improved from 0.23550 to 0.21395, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 4/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.2042 - acc: 0.9193 - val_loss: 0.2192 - val_acc: 0.9126\n",
      "\n",
      "Epoch 00004: loss improved from 0.21395 to 0.20421, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 5/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.2003 - acc: 0.9203 - val_loss: 0.2184 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00005: loss improved from 0.20421 to 0.20031, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 6/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1973 - acc: 0.9210 - val_loss: 0.2127 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00006: loss improved from 0.20031 to 0.19732, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 7/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1953 - acc: 0.9213 - val_loss: 0.2147 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00007: loss improved from 0.19732 to 0.19528, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 8/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1939 - acc: 0.9216 - val_loss: 0.2110 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00008: loss improved from 0.19528 to 0.19387, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 9/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1918 - acc: 0.9220 - val_loss: 0.2110 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00009: loss improved from 0.19387 to 0.19181, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 10/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1908 - acc: 0.9221 - val_loss: 0.2097 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00010: loss improved from 0.19181 to 0.19081, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 11/15\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.1897 - acc: 0.9223 - val_loss: 0.2101 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00011: loss improved from 0.19081 to 0.18970, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 12/15\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.1884 - acc: 0.9228 - val_loss: 0.2091 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00012: loss improved from 0.18970 to 0.18839, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 13/15\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.1873 - acc: 0.9229 - val_loss: 0.2081 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00013: loss improved from 0.18839 to 0.18733, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 14/15\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.1866 - acc: 0.9231 - val_loss: 0.2100 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00014: loss improved from 0.18733 to 0.18664, saving model to ./result_v9/unet.hdf5\n",
      "Epoch 15/15\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1847 - acc: 0.9235 - val_loss: 0.2092 - val_acc: 0.9148\n",
      "\n",
      "Epoch 00015: loss improved from 0.18664 to 0.18471, saving model to ./result_v9/unet.hdf5\n",
      "18/18 [==============================] - 0s 12ms/step\n",
      "[0.16636623421476948, 0.9319291677739885]\n",
      "('test_loss:', 0.16636623421476948)\n",
      "('test_acc:', 0.9319291677739885)\n",
      "Predict test data\n",
      "18/18 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "myunet = myUnet()\n",
    "myunet.run_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_predict = np.load('./'+result_folder+'/unet_predict.npy')\n",
    "imgs_test = np.load('./'+result_folder+'/unet_imgs_test.npy')\n",
    "imgs_mask_test = np.load('./'+result_folder+'/unet_imgs_mask_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('./'+result_folder+'/unet_predict.mat', {'img_predict':imgs_predict})\n",
    "sio.savemat('./'+result_folder+'/unet_imgs_test.mat', {'img_test':imgs_test})\n",
    "sio.savemat('./'+result_folder+'/unet_imgs_mask_test.mat', {'img_test':imgs_mask_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
